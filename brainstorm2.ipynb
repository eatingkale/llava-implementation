{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23c4310",
   "metadata": {},
   "source": [
    "# notes 2/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c89842",
   "metadata": {},
   "source": [
    "W is a very simple projection to take vision embeddings to language embedding space, but what other options are there? an entire network?\n",
    "\n",
    "they cite \"faster iteration of data centric experiments\" as the reason for keeping it simple - so that they could refine the instruction-following dataset that they produced\n",
    "\n",
    "questions:\n",
    "- does LLaVA also patch images?\n",
    "- what if the chosen vision embedding space and text embedding space naturally happened to be the same? then, would there be a need for any learning to happen? why?\n",
    "- are we re defining everything from scratch (doubtful) or like do we need to import llava model and tweak it? are there places we can specify the original llava vision tower (to be siglip) and the language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99dfec3",
   "metadata": {},
   "source": [
    "# dims of language and vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5dc86",
   "metadata": {},
   "source": [
    "### explore siglip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c833a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from transformers.image_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# this is taken directly from HF SigLIP2 page.\n",
    "\n",
    "# load the model and processor\n",
    "ckpt = \"google/siglip2-base-patch32-256\"\n",
    "siglip_encoder = AutoModel.from_pretrained(ckpt, device_map=\"auto\").eval() # device_map is not relevant for cpu. Also, we use AutoModel (not AutoModelFor..) because it directly outputs the hidden states from the encoder?\n",
    "siglip_processor = AutoProcessor.from_pretrained(ckpt)\n",
    "\n",
    "# load the image\n",
    "image = load_image(\"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\")\n",
    "inputs = siglip_processor(images=[image], return_tensors=\"pt\")#.to(siglip_encoder.device) # \n",
    "\n",
    "# run infernece\n",
    "with torch.no_grad():\n",
    "    image_embeddings = siglip_encoder.get_image_features(**inputs)    \n",
    "\n",
    "print(image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb8a57",
   "metadata": {},
   "source": [
    "notes on nn.Embedding (from docs):\n",
    "- A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "    - dictionary size 256,000?\n",
    "- This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "- Input: (∗), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "- Output: (∗,H), where * is the input shape and H=embedding_dim\n",
    "- https://rahullokurte.com/understanding-token-and-positional-embeddings-in-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f67d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo summarize: Siglip/LIP models in general have an image encoder and a text encoder. \\nVision Encoder: CLIP paper states that it chooses either (1) ViT or (2) Resnet50 \\nText Encoder: a modified Transformer, where \"the text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the\\n              highest layer of the transformer at the [EOS] token are treated as the feature representation of the text\\n              which is then normalized \\n              and then linearly projected into the multimodal embedding space.\\n\\n\\nSiglipModel: notes\\n- text model\\n    - embeddings: specifies positional + token (semantic) embeddings\\n        - Token Embeddings are (Vocab size, dims)\\n        - positional embeddings are (Sequence Length, dims)\\n    - encoder: specifies the model/encoder structure\\n        - 12 layers\\n        - out dimension: 768 (out_proj)\\n- vision model\\n    - vision embeddings: patch embeddings + positional embeddings\\n        - (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), padding=valid)\\n            - 3 in channels, 768 out channels\\n        -  (position_embedding): Embedding(64, 768)\\n    - \\n\\n\\n------- output of print(siglip_encoder)-------\\nSiglipModel(\\n  (text_model): SiglipTextTransformer(\\n    (embeddings): SiglipTextEmbeddings(\\n      (token_embedding): Embedding(256000, 768) # vocab size, dims\\n      (position_embedding): Embedding(64, 768) # sequence length, dims\\n    )\\n    (encoder): SiglipEncoder(\\n      (layers): ModuleList(\\n        (0-11): 12 x SiglipEncoderLayer(\\n          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n          (self_attn): SiglipAttention(\\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\\n          )\\n          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n          (mlp): SiglipMLP(\\n            (activation_fn): GELUTanh()\\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\\n          )\\n        )\\n      )\\n    )\\n    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n    (head): Linear(in_features=768, out_features=768, bias=True)\\n  )\\n  (vision_model): SiglipVisionTransformer(\\n    (embeddings): SiglipVisionEmbeddings(\\n      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), padding=valid)\\n      (position_embedding): Embedding(64, 768)\\n    )\\n    (encoder): SiglipEncoder(\\n      (layers): ModuleList(\\n        (0-11): 12 x SiglipEncoderLayer(\\n          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n          (self_attn): SiglipAttention(\\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\\n          )\\n          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n          (mlp): SiglipMLP(\\n            (activation_fn): GELUTanh()\\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\\n          )\\n        )\\n      )\\n    )\\n    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n    (head): SiglipMultiheadAttentionPoolingHead(\\n      (attention): MultiheadAttention(\\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\\n      )\\n      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\\n      (mlp): SiglipMLP(\\n        (activation_fn): GELUTanh()\\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\\n      )\\n    )\\n  )\\n)\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To summarize: Siglip/LIP models in general have an image encoder and a text encoder. \n",
    "Vision Encoder: CLIP paper states that it chooses either (1) ViT or (2) Resnet50 \n",
    "Text Encoder: a modified Transformer, where \"the text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the\n",
    "              highest layer of the transformer at the [EOS] token are treated as the feature representation of the text\n",
    "              which is then normalized \n",
    "              and then linearly projected into the multimodal embedding space.\n",
    "\n",
    "\n",
    "SiglipModel: notes\n",
    "- text model\n",
    "    - embeddings: specifies positional + token (semantic) embeddings\n",
    "        - Token Embeddings are (Vocab size, dims)\n",
    "        - positional embeddings are (Sequence Length, dims)\n",
    "    - encoder: specifies the model/encoder structure\n",
    "        - 12 layers\n",
    "        - out dimension: 768 (out_proj)\n",
    "- vision model\n",
    "    - vision embeddings: patch embeddings + positional embeddings\n",
    "        - (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), padding=valid)\n",
    "            - 3 in channels, 768 out channels\n",
    "        -  (position_embedding): Embedding(64, 768)\n",
    "    - \n",
    "\"\"\"\n",
    "# print(siglip_encoder)\n",
    "# ------- output of print(siglip_encoder)-------\n",
    "\"\"\"\n",
    "SiglipModel(\n",
    "  (text_model): SiglipTextTransformer(\n",
    "    (embeddings): SiglipTextEmbeddings(\n",
    "      (token_embedding): Embedding(256000, 768) # vocab size, dims\n",
    "      (position_embedding): Embedding(64, 768) # sequence length, dims\n",
    "    )\n",
    "    (encoder): SiglipEncoder(\n",
    "      (layers): ModuleList(\n",
    "        (0-11): 12 x SiglipEncoderLayer(\n",
    "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "          (self_attn): SiglipAttention(\n",
    "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "          )\n",
    "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "          (mlp): SiglipMLP(\n",
    "            (activation_fn): GELUTanh()\n",
    "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
    "  )\n",
    "  (vision_model): SiglipVisionTransformer(\n",
    "    (embeddings): SiglipVisionEmbeddings(\n",
    "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), padding=valid)\n",
    "      (position_embedding): Embedding(64, 768)\n",
    "    )\n",
    "    (encoder): SiglipEncoder(\n",
    "      (layers): ModuleList(\n",
    "        (0-11): 12 x SiglipEncoderLayer(\n",
    "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "          (self_attn): SiglipAttention(\n",
    "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "          )\n",
    "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "          (mlp): SiglipMLP(\n",
    "            (activation_fn): GELUTanh()\n",
    "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "    (head): SiglipMultiheadAttentionPoolingHead(\n",
    "      (attention): MultiheadAttention(\n",
    "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
    "      )\n",
    "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): SiglipMLP(\n",
    "        (activation_fn): GELUTanh()\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c51475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(siglip_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11557941",
   "metadata": {},
   "source": [
    "### explore LFM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1e6f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"LiquidAI/LFM2-350M\"\n",
    "lfm2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d10b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|><|im_start|>user\n",
      "What is C. elegans?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "C. elegans, commonly known as the razorback slug or zebra mussel, is a small, transparent, and fast-moving nematode worm (roundworm) species from the genus C. elegans. It was first discovered in 1856 by German entomologist Hans Christian Ørsted and is named after Danish mathematician Carl Friedrich Wilhelm Emanuel Elyseus E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E. E\n"
     ]
    }
   ],
   "source": [
    "# Generate answer\n",
    "prompt = \"What is C. elegans?\"\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    ").to(lfm2.device)\n",
    "\n",
    "output = lfm2.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    min_p=0.15,\n",
    "    repetition_penalty=1.05,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a49a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lfm2ForCausalLM(\n",
      "  (model): Lfm2Model(\n",
      "    (embed_tokens): Embedding(65536, 1024, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (2): Lfm2DecoderLayer(\n",
      "        (self_attn): Lfm2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (3-4): 2 x Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (5): Lfm2DecoderLayer(\n",
      "        (self_attn): Lfm2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (6-7): 2 x Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (8): Lfm2DecoderLayer(\n",
      "        (self_attn): Lfm2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (9): Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (10): Lfm2DecoderLayer(\n",
      "        (self_attn): Lfm2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (11): Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (12): Lfm2DecoderLayer(\n",
      "        (self_attn): Lfm2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (13): Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (14): Lfm2DecoderLayer(\n",
      "        (self_attn): Lfm2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "      (15): Lfm2DecoderLayer(\n",
      "        (conv): Lfm2ShortConv(\n",
      "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
      "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (feed_forward): Lfm2MLP(\n",
      "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
      "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
      "        )\n",
      "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (rotary_emb): Lfm2RotaryEmbedding()\n",
      "    (pos_emb): Lfm2RotaryEmbedding()\n",
      "    (embedding_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=65536, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nSince the in dimension of the lfm2 model is 1024, perhaps then the dimension that we need to transfer to \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lfm2)\n",
    "\n",
    "\"\"\" \n",
    "Since the in dimension of the lfm2 model is 1024, perhaps then the dimension that we need to transfer to \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300f4ea",
   "metadata": {},
   "source": [
    "### explore llava"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d57a4",
   "metadata": {},
   "source": [
    "# structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ce0c2",
   "metadata": {},
   "source": [
    "1. dataset loading - do we define a custom class?\n",
    "    - also note that the images have to be married into the dataset, at least from the link to the IF dataset.\n",
    "2. dataloader\n",
    "3. define model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e0082",
   "metadata": {},
   "source": [
    "input: image + text prompt (question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ef079",
   "metadata": {},
   "source": [
    "# examine llava docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cda74a",
   "metadata": {},
   "source": [
    "Running the example code from https://huggingface.co/docs/transformers/en/model_doc/llava to see what it does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145de5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "# the below code snippet comes directly from the llava docs above\n",
    "\n",
    "# LlavaConfig\n",
    "from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig\n",
    "\n",
    "# Initializing a CLIP-vision config\n",
    "vision_config = CLIPVisionConfig()\n",
    "\n",
    "# Initializing a Llama config\n",
    "text_config = LlamaConfig()\n",
    "\n",
    "# Initializing a Llava llava-1.5-7b style configuration\n",
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "\n",
    "# Initializing a model from the llava-1.5-7b style configuration\n",
    "model = LlavaForConditionalGeneration(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71b450",
   "metadata": {},
   "source": [
    "^ this takes forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62e84f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe10b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm2_config_dict = lfm2.config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afe8d0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 65536,\n",
       " 'hidden_size': 1024,\n",
       " 'num_hidden_layers': 16,\n",
       " 'rope_theta': 1000000.0,\n",
       " 'max_position_embeddings': 128000,\n",
       " 'use_cache': True,\n",
       " 'norm_eps': 1e-05,\n",
       " 'initializer_range': 0.02,\n",
       " 'num_attention_heads': 16,\n",
       " 'num_key_value_heads': 8,\n",
       " 'conv_bias': False,\n",
       " 'conv_L_cache': 3,\n",
       " 'intermediate_size': 6656,\n",
       " 'block_multiple_of': 256,\n",
       " 'block_ffn_dim_multiplier': 1.0,\n",
       " 'block_auto_adjust_ff_dim': True,\n",
       " 'layer_types': ['conv',\n",
       "  'conv',\n",
       "  'full_attention',\n",
       "  'conv',\n",
       "  'conv',\n",
       "  'full_attention',\n",
       "  'conv',\n",
       "  'conv',\n",
       "  'full_attention',\n",
       "  'conv',\n",
       "  'full_attention',\n",
       "  'conv',\n",
       "  'full_attention',\n",
       "  'conv',\n",
       "  'full_attention',\n",
       "  'conv'],\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'torchscript': False,\n",
       " 'dtype': 'bfloat16',\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'architectures': ['Lfm2ForCausalLM'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': 1,\n",
       " 'pad_token_id': 0,\n",
       " 'eos_token_id': 7,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " '_name_or_path': 'LiquidAI/LFM2-350M',\n",
       " 'transformers_version': '4.57.6',\n",
       " 'block_dim': 1024,\n",
       " 'block_ff_dim': 6656,\n",
       " 'block_mlp_init_scale': 1.0,\n",
       " 'block_norm_eps': 1e-05,\n",
       " 'block_out_init_scale': 1.0,\n",
       " 'block_use_swiglu': True,\n",
       " 'block_use_xavier_init': True,\n",
       " 'conv_dim': 1024,\n",
       " 'conv_dim_out': 1024,\n",
       " 'conv_use_xavier_init': True,\n",
       " 'model_type': 'lfm2',\n",
       " 'num_heads': 16,\n",
       " 'use_pos_enc': True,\n",
       " 'tf_legacy_loss': False,\n",
       " 'use_bfloat16': False,\n",
       " 'output_attentions': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfm2_config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a19a9803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type siglip_vision_model to instantiate a model of type siglip2_vision_model. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# Use lfm2 config and siglip\n",
    "\n",
    "# from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig\n",
    "from transformers import Lfm2Config, Siglip2VisionConfig, AutoConfig\n",
    "\n",
    "# Initializing a siglip vision config\n",
    "vision_config = Siglip2VisionConfig().from_pretrained(\"google/siglip2-base-patch32-256\")\n",
    "\n",
    "# Initializing a lfm2 350m config hopefully\n",
    "text_config = AutoConfig.from_pretrained(\"LiquidAI/LFM2-350M\")\n",
    "\n",
    "# Initializing a Llava llava-1.5-7b style configuration\n",
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "\n",
    "# Initializing a model from the llava-1.5-7b style configuration\n",
    "llava_model = LlavaForConditionalGeneration(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = llava_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad920c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaForConditionalGeneration(\n",
       "  (model): LlavaModel(\n",
       "    (vision_tower): Siglip2VisionModel(\n",
       "      (vision_model): Siglip2VisionTransformer(\n",
       "        (embeddings): Siglip2VisionEmbeddings(\n",
       "          (patch_embedding): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (position_embedding): Embedding(256, 768)\n",
       "        )\n",
       "        (encoder): Siglip2Encoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x Siglip2EncoderLayer(\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): Siglip2Attention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Siglip2MLP(\n",
       "                (activation_fn): GELUTanh()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (head): Siglip2MultiheadAttentionPoolingHead(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Siglip2MLP(\n",
       "            (activation_fn): GELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): LlavaMultiModalProjector(\n",
       "      (linear_1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "      (act): GELUActivation()\n",
       "      (linear_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (language_model): Lfm2Model(\n",
       "      (embed_tokens): Embedding(65536, 1024, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (2): Lfm2DecoderLayer(\n",
       "          (self_attn): Lfm2Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (3-4): 2 x Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (5): Lfm2DecoderLayer(\n",
       "          (self_attn): Lfm2Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (6-7): 2 x Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (8): Lfm2DecoderLayer(\n",
       "          (self_attn): Lfm2Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (9): Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (10): Lfm2DecoderLayer(\n",
       "          (self_attn): Lfm2Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (11): Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (12): Lfm2DecoderLayer(\n",
       "          (self_attn): Lfm2Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (13): Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (14): Lfm2DecoderLayer(\n",
       "          (self_attn): Lfm2Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "            (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "        (15): Lfm2DecoderLayer(\n",
       "          (conv): Lfm2ShortConv(\n",
       "            (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (feed_forward): Lfm2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "            (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "          (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (rotary_emb): Lfm2RotaryEmbedding()\n",
       "      (pos_emb): Lfm2RotaryEmbedding()\n",
       "      (embedding_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=65536, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "727e3f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaModel(\n",
       "  (vision_tower): Siglip2VisionModel(\n",
       "    (vision_model): Siglip2VisionTransformer(\n",
       "      (embeddings): Siglip2VisionEmbeddings(\n",
       "        (patch_embedding): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (position_embedding): Embedding(256, 768)\n",
       "      )\n",
       "      (encoder): Siglip2Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Siglip2EncoderLayer(\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): Siglip2Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Siglip2MLP(\n",
       "              (activation_fn): GELUTanh()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Siglip2MultiheadAttentionPoolingHead(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Siglip2MLP(\n",
       "          (activation_fn): GELUTanh()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (language_model): Lfm2Model(\n",
       "    (embed_tokens): Embedding(65536, 1024, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (2): Lfm2DecoderLayer(\n",
       "        (self_attn): Lfm2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (3-4): 2 x Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (5): Lfm2DecoderLayer(\n",
       "        (self_attn): Lfm2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (6-7): 2 x Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (8): Lfm2DecoderLayer(\n",
       "        (self_attn): Lfm2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (9): Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (10): Lfm2DecoderLayer(\n",
       "        (self_attn): Lfm2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (11): Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (12): Lfm2DecoderLayer(\n",
       "        (self_attn): Lfm2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (13): Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (14): Lfm2DecoderLayer(\n",
       "        (self_attn): Lfm2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "      (15): Lfm2DecoderLayer(\n",
       "        (conv): Lfm2ShortConv(\n",
       "          (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), groups=1024, bias=False)\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (feed_forward): Lfm2MLP(\n",
       "          (w1): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w3): Linear(in_features=1024, out_features=4608, bias=False)\n",
       "          (w2): Linear(in_features=4608, out_features=1024, bias=False)\n",
       "        )\n",
       "        (operator_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "        (ffn_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): Lfm2RotaryEmbedding()\n",
       "    (pos_emb): Lfm2RotaryEmbedding()\n",
       "    (embedding_norm): Lfm2RMSNorm((1024,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc8fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffcc0cc1",
   "metadata": {},
   "source": [
    "# Transformers Documentation: Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438da82",
   "metadata": {},
   "source": [
    "- Every model is implemented from only three main classes (configuration, model, and preprocessor).\n",
    "- The number of user-facing abstractions is limited to only three classes for instantiating a model, and two APIs for inference or training.\n",
    "- Model gets initialized from a config\n",
    "\n",
    "- https://huggingface.co/docs/transformers/en/how_to_hack_models customizing models\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fac55c",
   "metadata": {},
   "source": [
    "# identifying the complete workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc82436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nX_v image: .png/.jpg, local or remote\\n|\\n| jpg -> Image: load image using PIL to get an Image\\n| Image -> Tensor: \\n| \\n| do we use the Siglip Processor here? (autoprocessor??)\\nV\\nVision Encoder: input needs to be a tensor\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "X_v image: .png/.jpg, local or remote\n",
    "|\n",
    "| jpg -> Image: load image using PIL to get an Image\n",
    "| Image -> Tensor: \n",
    "| \n",
    "| do we use the Siglip Processor here? (autoprocessor??)\n",
    "V\n",
    "Vision Encoder: input needs to be a tensor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746237d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
